# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
# This workflow builds GPU packages on GitHub-hosted runners (ubuntu-latest)
# to verify CUDA compilation works on Linux.
#
# GPU tests are commented out (no GPU hardware on GitHub runners).
# Once a self-hosted GPU runner is available, uncomment test steps.
# See docs/setup-gpu-ci-runner.md for GPU runner setup.
#
name: rust-gpu

on:
  push:
    branches:
      - main
      - feature/**
    paths:
      - 'c/sedona-libgpuspatial/**'
      - 'rust/sedona-spatial-join-gpu/**'
      - '.github/workflows/rust.yml'

concurrency:
  group: ${{ github.repository }}-${{ github.ref }}-${{ github.workflow }}-rust-gpu
  cancel-in-progress: true

# Set workflow timeout to 60 minutes (default is 360 minutes/6 hours)
# GPU build takes ~30-40 minutes first time, ~5-8 minutes cached
env:
  WORKFLOW_TIMEOUT_MINUTES: 60

permissions:
  contents: read

defaults:
  run:
    shell: bash -l -eo pipefail {0}

jobs:
  rust-gpu-build:
    name: "GPU Build with CUDA (tests disabled, no GPU hardware)"
    # Using GitHub-hosted runner to verify CUDA compilation
    # TODO: Once GPU runner is ready, either:
    #   1. Update this job: runs-on: [self-hosted, gpu, linux, cuda]
    #   2. Or create separate GPU test job with GPU runner
    # Recommended GPU runner: AWS EC2 g5.xlarge with Deep Learning AMI
    runs-on: ubuntu-latest
    timeout-minutes: 60  # Prevent runaway builds (first build ~40min, cached ~8min)

    env:
      CARGO_INCREMENTAL: 0
      # Reduce debug info to save disk space
      CARGO_PROFILE_DEV_DEBUG: 1
      CARGO_PROFILE_TEST_DEBUG: 1
      # Limit parallel compilation to reduce memory pressure (GPU compilation is intensive)
      # Adjust based on instance size: 4 for g5.xlarge, 8 for g5.2xlarge
      CARGO_BUILD_JOBS: 4
      # CUDA configuration
      CUDA_HOME: /usr/local/cuda
      # vcpkg for C++ dependencies (version-pinned for reproducibility)
      VCPKG_ROOT: ${{ github.workspace }}/vcpkg
      CMAKE_TOOLCHAIN_FILE: ${{ github.workspace }}/vcpkg/scripts/buildsystems/vcpkg.cmake

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'

      # Clone vcpkg with specific version for reproducibility
      # Based on libgpuspatial CI best practices
      - name: Clone vcpkg
        uses: actions/checkout@v4
        with:
          repository: microsoft/vcpkg
          ref: "2025.06.13"
          path: vcpkg
          fetch-depth: 0

      # Set up environment variables for vcpkg and CUDA
      - name: Bootstrap vcpkg and set environment
        run: |
          cd vcpkg
          ./bootstrap-vcpkg.sh
          cd ..

          echo "VCPKG_ROOT=$VCPKG_ROOT" >> $GITHUB_ENV
          echo "PATH=$VCPKG_ROOT:$PATH" >> $GITHUB_ENV
          echo "CMAKE_TOOLCHAIN_FILE=$CMAKE_TOOLCHAIN_FILE" >> $GITHUB_ENV
          echo "/usr/local/cuda/bin" >> $GITHUB_PATH

      # Install system dependencies including CUDA toolkit
      - name: Install system dependencies
        run: |
          sudo apt-get update

          # Install transport tools for Kitware CMake repository
          sudo apt-get install -y apt-transport-https ca-certificates gnupg software-properties-common wget

          # Add Kitware repository for latest CMake (required for CUDA support)
          wget -qO - https://apt.kitware.com/keys/kitware-archive-latest.asc | sudo apt-key add -
          sudo apt-add-repository 'deb https://apt.kitware.com/ubuntu/ jammy main'
          sudo apt-get update

          # Install build tools
          sudo apt-get install -y build-essential pkg-config cmake flex bison

          # Install GEOS for spatial operations
          sudo apt-get install -y libgeos-dev libzstd-dev

          # Install CUDA toolkit
          # Note: GitHub runners may have CUDA pre-installed, check first
          if ! command -v nvcc &> /dev/null; then
            echo "Installing CUDA toolkit..."
            sudo apt-get install -y nvidia-cuda-toolkit
          else
            echo "CUDA toolkit already installed: $(nvcc --version)"
          fi

      # Cache vcpkg installed packages (expensive to rebuild)
      - name: Cache vcpkg binaries
        id: cache-vcpkg
        uses: actions/cache@v4
        with:
          path: ${{ github.workspace }}/vcpkg_installed
          # Increment version suffix to force rebuild
          key: vcpkg-installed-${{ runner.os }}-${{ runner.arch }}-1

      # Install vcpkg dependencies if not cached
      - name: Install vcpkg dependencies
        if: steps.cache-vcpkg.outputs.cache-hit != 'true'
        run: |
          echo "Installing vcpkg dependencies (abseil, openssl)..."
          ./vcpkg/vcpkg install abseil openssl

      - name: Setup Rust toolchain
        run: |
          rustup toolchain install stable --no-self-update
          rustup default stable
          cargo --version
          rustc --version

      - uses: Swatinem/rust-cache@v2
        with:
          prefix-key: "rust-gpu-v3"
          # Cache key includes GPU packages and vcpkg config
          key: "${{ runner.os }}-${{ hashFiles('c/sedona-libgpuspatial/**', 'vcpkg.json') }}"

      # Build WITH GPU feature to verify CUDA compilation
      - name: Build libgpuspatial (with GPU/CUDA support)
        run: |
          echo "=== Building libgpuspatial WITH GPU feature ==="
          echo "This compiles CUDA code and verifies CUDA compilation works"
          echo "Note: First build with CUDA takes approximately 20-25 minutes"
          echo ""
          # Use --locked to ensure Cargo.lock is up to date
          # Use --all-targets to build lib + tests (but don't run tests)
          cargo build --locked --package sedona-libgpuspatial --all-targets --features gpu --verbose
          echo ""
          echo "✓ libgpuspatial build complete with CUDA support"

      - name: Build GPU spatial join (with GPU feature)
        run: |
          echo "=== Building GPU spatial join package WITH GPU feature ==="
          # Build only GPU packages, not entire workspace (saves time)
          cargo build --locked --package sedona-spatial-join-gpu --all-targets --features gpu --verbose
          echo ""
          echo "✓ GPU spatial join build complete"

      # GPU tests commented out - no GPU hardware on GitHub runners
      # Uncomment these when running on self-hosted GPU runner

      # - name: Test libgpuspatial
      #   run: |
      #     echo "Running libgpuspatial tests with GPU..."
      #     cargo test --package sedona-libgpuspatial --features gpu -- --nocapture

      # - name: Test GPU spatial join (structure tests)
      #   run: |
      #     echo "Running structure tests (don't require GPU execution)..."
      #     cargo test --package sedona-spatial-join-gpu --features gpu

      # - name: Test GPU functional tests (require GPU)
      #   run: |
      #     echo "Running GPU functional tests (require actual GPU)..."
      #     cargo test --package sedona-spatial-join-gpu --features gpu -- --ignored --nocapture

      - name: Build summary
        run: |
          echo "=== GPU Package Build Summary ==="
          echo "Runner: GitHub-hosted (ubuntu-latest)"
          echo "Build Mode: WITH GPU feature (--features gpu)"
          echo "CUDA Compilation: ✓ Verified"
          echo "Platform: $(uname -a)"
          echo "Rust Version: $(rustc --version)"
          if command -v nvcc &> /dev/null; then
            echo "CUDA Version: $(nvcc --version | grep release)"
          fi
          echo ""
          echo "✓ GPU packages built successfully with CUDA support"
          echo "✓ CUDA compilation verified on Linux"
          echo "⚠ GPU tests skipped (no GPU hardware on GitHub runners)"
          echo ""
          echo "Next steps to enable GPU testing:"
          echo "  1. Set up self-hosted runner with GPU (see docs/setup-gpu-ci-runner.md)"
          echo "  2. Update runs-on to: [self-hosted, gpu, linux, cuda]"
          echo "  3. Uncomment GPU test steps in this workflow"
          echo "  4. Verify tests pass with actual GPU hardware"