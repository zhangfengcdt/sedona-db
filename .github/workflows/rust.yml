# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
# This workflow compiles CUDA code on GitHub-hosted runners (ubuntu-22.04).
# CUDA compilation (nvcc) works WITHOUT GPU hardware - only needs CUDA toolkit.
# GPU runtime execution requires actual GPU, so tests are commented out.
#
# Environment: ubuntu-22.04 (matches libgpuspatial CI for consistency)
#
# What this workflow tests:
#   ✓ CUDA code compilation (nvcc)
#   ✓ C++/CUDA CMake build (with CMAKE_POLICY_VERSION_MINIMUM=3.5)
#   ✓ Rust/C++ FFI bindings
#   ✓ Linux compatibility
#
# What this workflow does NOT test:
#   ✗ GPU kernel execution (requires GPU hardware)
#   ✗ GPU memory operations (requires GPU drivers)
#   ✗ Spatial join correctness on GPU
#
# Build time: ~45-60 minutes first time, ~10-15 minutes with cache
#
# See docs/setup-gpu-ci-runner.md for self-hosted GPU runner setup.
#
name: rust-gpu

on:
  push:
    branches:
      - main
      - feature/**
    paths:
      - 'c/sedona-libgpuspatial/**'
      - 'rust/sedona-spatial-join-gpu/**'
      - '.github/workflows/rust.yml'

concurrency:
  group: ${{ github.repository }}-${{ github.ref }}-${{ github.workflow }}-rust-gpu
  cancel-in-progress: true

# Set workflow timeout to 90 minutes for CUDA compilation
# Expected: ~45-60 minutes first time, ~10-15 minutes cached
env:
  WORKFLOW_TIMEOUT_MINUTES: 90

permissions:
  contents: read

defaults:
  run:
    shell: bash -l -eo pipefail {0}

jobs:
  rust-gpu-build:
    name: "GPU CUDA Compilation (no GPU runtime/tests)"
    # Using GitHub-hosted runner to compile CUDA code
    # CUDA compilation works without GPU hardware (only needs CUDA toolkit)
    # GPU tests are skipped (no GPU hardware for runtime execution)
    # Using ubuntu-22.04 to match libgpuspatial CI environment
    # TODO: Once GPU runner is ready, enable GPU tests with:
    #   runs-on: [self-hosted, gpu, linux, cuda]
    # Recommended GPU runner: AWS EC2 g5.xlarge with Deep Learning AMI
    runs-on: ubuntu-22.04
    timeout-minutes: 90  # CUDA compilation is slow (~45-60 min first time)

    env:
      CARGO_INCREMENTAL: 0
      # Reduce debug info to save disk space
      CARGO_PROFILE_DEV_DEBUG: 1
      CARGO_PROFILE_TEST_DEBUG: 1
      # Limit parallel compilation to reduce memory pressure (GPU compilation is intensive)
      # Adjust based on instance size: 4 for g5.xlarge, 8 for g5.2xlarge
      CARGO_BUILD_JOBS: 4
      # CUDA configuration
      CUDA_HOME: /usr/local/cuda
      # vcpkg for C++ dependencies (version-pinned for reproducibility)
      VCPKG_ROOT: ${{ github.workspace }}/vcpkg
      CMAKE_TOOLCHAIN_FILE: ${{ github.workspace }}/vcpkg/scripts/buildsystems/vcpkg.cmake

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'

      # Clone vcpkg with specific version for reproducibility
      # Based on libgpuspatial CI best practices
      - name: Clone vcpkg
        uses: actions/checkout@v4
        with:
          repository: microsoft/vcpkg
          ref: "2025.06.13"
          path: vcpkg
          fetch-depth: 0

      # Set up environment variables for vcpkg and CUDA
      - name: Bootstrap vcpkg and set environment
        run: |
          cd vcpkg
          ./bootstrap-vcpkg.sh
          cd ..

          echo "VCPKG_ROOT=$VCPKG_ROOT" >> $GITHUB_ENV
          echo "PATH=$VCPKG_ROOT:$PATH" >> $GITHUB_ENV
          echo "CMAKE_TOOLCHAIN_FILE=$CMAKE_TOOLCHAIN_FILE" >> $GITHUB_ENV
          echo "/usr/local/cuda/bin" >> $GITHUB_PATH

      # Free up disk space before build (GitHub runners have limited space)
      - name: Free disk space
        run: |
          echo "Disk space before cleanup:"
          df -h

          # Remove unnecessary packages to free up ~10GB
          sudo apt-get remove -y '^dotnet-.*' '^llvm-.*' 'php.*' '^mongodb-.*' '^mysql-.*' azure-cli google-chrome-stable firefox powershell mono-devel libgl1-mesa-dri || true
          sudo apt-get autoremove -y
          sudo apt-get clean

          # Remove large directories
          sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /opt/hostedtoolcache/CodeQL || true

          echo "Disk space after cleanup:"
          df -h

      # Install system dependencies including CUDA toolkit for compilation
      # Following libgpuspatial CI pattern
      - name: Install system dependencies
        run: |
          sudo apt-get update

          # Install transport tools for Kitware CMake (needed for newer CMake)
          sudo apt-get install -y apt-transport-https ca-certificates gnupg software-properties-common wget

          # Add Kitware repository for CMake
          wget -qO - https://apt.kitware.com/keys/kitware-archive-latest.asc | sudo apt-key add -
          sudo apt-add-repository 'deb https://apt.kitware.com/ubuntu/ jammy main'
          sudo apt-get update

          # Install build tools
          sudo apt-get install -y build-essential pkg-config cmake flex bison

          # Install libclang for bindgen (Rust FFI binding generator)
          sudo apt-get install -y libclang-dev

          # Verify compiler and CMake versions
          gcc --version
          g++ --version
          cmake --version

          # Install GEOS for spatial operations
          sudo apt-get install -y libgeos-dev libzstd-dev

          # Install CUDA toolkit for compilation (nvcc)
          # Note: CUDA compilation works without GPU hardware
          # GPU runtime tests still require actual GPU
          # Use CUDA 12.4 to match libgpuspatial CI environment
          # CUDA 12.4 supports GCC 11 (default on Ubuntu 22.04)
          if ! command -v nvcc &> /dev/null; then
            echo "Installing CUDA 12.4 toolkit for compilation..."

            # Add NVIDIA CUDA repository
            wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
            sudo dpkg -i cuda-keyring_1.1-1_all.deb
            sudo apt-get update

            # Remove any existing CUDA toolkit
            sudo apt purge cuda-toolkit* -y || true

            # Install CUDA 12.4 specifically
            sudo apt-get install -y cuda-toolkit-12-4

            # Set CUDA path
            echo "/usr/local/cuda-12.4/bin" >> $GITHUB_PATH

            nvcc --version
          else
            echo "CUDA toolkit already installed: $(nvcc --version)"
          fi

      # Cache vcpkg installed packages (expensive to rebuild)
      - name: Cache vcpkg binaries
        id: cache-vcpkg
        uses: actions/cache@v4
        with:
          path: ${{ github.workspace }}/vcpkg_installed
          # Increment version suffix to force rebuild
          key: vcpkg-installed-${{ runner.os }}-${{ runner.arch }}-1

      # Install vcpkg dependencies from vcpkg.json manifest
      - name: Install vcpkg dependencies
        if: steps.cache-vcpkg.outputs.cache-hit != 'true'
        run: |
          echo "Installing vcpkg dependencies from vcpkg.json..."
          echo "Manifest: c/sedona-libgpuspatial/libgpuspatial/vcpkg.json"
          echo "Dependencies: nlohmann-json, gflags, gtest, zstd, arrow[s3]"
          # Use --x-manifest-root to specify the directory with vcpkg.json
          ./vcpkg/vcpkg install --x-manifest-root=c/sedona-libgpuspatial/libgpuspatial

      - name: Setup Rust toolchain
        run: |
          rustup toolchain install stable --no-self-update
          rustup default stable
          cargo --version
          rustc --version

      - uses: Swatinem/rust-cache@v2
        with:
          prefix-key: "rust-gpu-v3"
          # Cache key includes GPU packages and vcpkg config
          key: "${{ runner.os }}-${{ hashFiles('c/sedona-libgpuspatial/**', 'vcpkg.json') }}"

      # Build WITH GPU feature to compile CUDA code
      # CUDA compilation (nvcc) works without GPU hardware
      # Only GPU runtime execution requires actual GPU
      - name: Build libgpuspatial (with CUDA compilation)
        run: |
          echo "=== Building libgpuspatial WITH GPU feature ==="
          echo "Compiling CUDA code using nvcc (no GPU hardware needed for compilation)"
          echo "Note: First build with CUDA takes 45-60 minutes (CMake + CUDA compilation)"
          echo "Subsequent builds: 10-15 minutes (cached)"
          echo ""
          echo "Build started at: $(date)"
          # Build with --features gpu to trigger CUDA compilation
          cargo build --locked --package sedona-libgpuspatial --all-targets --features gpu --verbose
          echo ""
          echo "Build completed at: $(date)"
          echo "✓ libgpuspatial CUDA compilation complete"

      - name: Build GPU spatial join (with GPU feature)
        run: |
          echo "=== Building GPU spatial join package WITH GPU feature ==="
          echo "Building Rust GPU spatial join (depends on libgpuspatial)"
          echo ""
          cargo build --locked --package sedona-spatial-join-gpu --all-targets --features gpu --verbose
          echo ""
          echo "✓ GPU spatial join build complete"

      # GPU tests commented out - no GPU hardware on GitHub runners
      # Uncomment these when running on self-hosted GPU runner

      # - name: Test libgpuspatial
      #   run: |
      #     echo "Running libgpuspatial tests with GPU..."
      #     cargo test --package sedona-libgpuspatial --features gpu -- --nocapture

      # - name: Test GPU spatial join (structure tests)
      #   run: |
      #     echo "Running structure tests (don't require GPU execution)..."
      #     cargo test --package sedona-spatial-join-gpu --features gpu

      # - name: Test GPU functional tests (require GPU)
      #   run: |
      #     echo "Running GPU functional tests (require actual GPU)..."
      #     cargo test --package sedona-spatial-join-gpu --features gpu -- --ignored --nocapture

      - name: Build summary
        run: |
          echo "=== GPU Package Build Summary ==="
          echo "Runner: GitHub-hosted (ubuntu-latest)"
          echo "Build Mode: WITH GPU feature (--features gpu)"
          echo "CUDA Compilation: ✓ Completed"
          echo "Platform: $(uname -a)"
          echo "Rust Version: $(rustc --version)"
          if command -v nvcc &> /dev/null; then
            echo "CUDA Version: $(nvcc --version | grep release)"
          fi
          echo ""
          echo "✓ CUDA code compiled successfully (nvcc)"
          echo "✓ libgpuspatial C++/CUDA build complete"
          echo "✓ GPU spatial join package built"
          echo "✓ All dependencies resolved"
          echo "⚠ GPU runtime tests NOT run (no GPU hardware)"
          echo ""
          echo "What was tested:"
          echo "  • CUDA code compilation (nvcc)"
          echo "  • C++/CUDA CMake build"
          echo "  • Rust/C++ FFI bindings"
          echo "  • Conditional compilation (#[cfg(feature = \"gpu\")])"
          echo "  • Linux compatibility"
          echo ""
          echo "What was NOT tested:"
          echo "  • GPU kernel execution (requires GPU hardware)"
          echo "  • GPU memory operations (requires GPU drivers)"
          echo "  • Spatial join correctness on GPU"
          echo "  • Performance characteristics"
          echo ""
          echo "Next steps to enable GPU runtime tests:"
          echo "  1. Set up self-hosted runner with GPU (see docs/setup-gpu-ci-runner.md)"
          echo "  2. Update runs-on to: [self-hosted, gpu, linux, cuda]"
          echo "  3. Uncomment GPU test steps in this workflow"
          echo "  4. Run tests with actual GPU hardware"